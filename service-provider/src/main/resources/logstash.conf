# 文件保证是 utf-8 的编码 不然启动logstash会报错


input {
	kafka {
		codec => json {
			charset => "UTF-8"
		}
		topics => "test"
		bootstrap_servers => "192.168.119.128:9092"  #这里最好写多个
		consumer_threads => 1  #这个线程数和分片数对应  一个分片只能有一个consumer 多的consumer也不能获取到数据
		auto_offset_reset => "earliest" # 当保存的offset找不到时的处理方案  移动到当前最小的offset进行处理    有重复消费的风险  写入es通过id保证幂等性 问题不大
	}
	http {
		  host => "172.18.23.131" # default: 0.0.0.0
		  port => 8089 # default: 8080
		  codec => "json"
	}
	beats{
		  port => 5045
	}
}

filter {
	mutate{
		remove_field => ["source","tags","beat","@version","offset","input_type","type"]
  	}
	json{
		source => "message" #这种方式解析json 后message还是存在的 最好的方法还是设置 kakfka consumer value deserializer class
	}
}

output {
  elasticsearch {
    hosts => "172.18.21.140:9200"
    index => "kafka-data"
    #document_id  => "110" #指定写入es后 文档的id
    codec => "json"
  }
}